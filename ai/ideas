<h1>Semantic Clustersing with Mutual Information</h1>

<h2>The Plan</h2>

<p>take lots of sentences.. for each pair of words (e.g. using a
sliding window that looks at 2 words at a time), record a frequency.
E.g. given the sentence "how would you model meme clusters", break
into word pairs:</p>

  <blockquote>"how would", "would you", "you model", ... </blockquote>

<p>each time you see a pair, you increment the count for that pair.</p>

<p>You then derive a probability from the counts.</p>

<p>And the Entropy of the pair is then the log probability.</p>

<p>Pairs with higher entropy get strong links.. so if you're trying to
decide which "sticks" more: "how would" or "would you", you see which
has higher Mutual Information, as it's called.  the latter probably
does.  So you make a link, and treat "would you" as a single unit.
then it becomes something other fragments in the sentence can stick
to.  Proceed "upwards", and you get sentence parses.  Higher still and
you get...?  Memes?</p>

<hr>
Original package.html

<p>This is an attempt at a general Artificial Intelligence using a
network of Wolfram's Automata as a neural fabric, which is built
incrementally based on action/feedback loops guided by Cziko's
description of perceptual control theory.</p>

<p>The choice of a graph of Wolfram Automata as a neural fabric reflects
the intended inductive bias of the learner, which is a simple
restatement of Wolfram's claim about pattern:</p>

<blockquote>
  All patterns can be represented by the action of dataflow through a
  network of wolfram automata.
</blockquote>

<p>If this is true, then learning a model of such a network becomes the
task of imitating its structure given only the knowledge of its
output.  Schematically:</p>

<pre>
           Generator   Learner
           a        #  ?   ?   
            \       #   \ /     Signal
Signal       a-a a  #    a-?    consumed
generated    |/  |  #    |/     and pattern
downwards  a-a   a  #  a-a   ?  guessed
    |      | |\ /|  #  | |\ /|  upwards
    |      a a a a  #  a a a a     ^
    v               #              |
           | | | |     ^ ^ ^ ^
           v v v v     | | | |

    t3     0 0 1 0     0 1 1 0     t1
    t2     1 0 1 1     1 0 1 1     t2
    t1     0 1 1 0     0 0 1 0     t3
           ------------------>
            Generated signal
            fed into learner.
</pre>

<p>Seen from the learner's perspective, the goal is to build a network of
bit transformations that is capable of producing the best possible
prediction of the source's signal, given practical time and space
constraints.  The learner does this by incrementally building a
structure that autoencodes the output signal of the source.</p>

<p>Learning proceeds by approximation.  Raw input data is either exactly
encoded or approximated during an upwards pass.  Potentially, patterns
that were too hard or impossible to predict upwards are attempted by
guesses at possible context dependencies on a downwards pass. (?)</p>

<p>More about PCT and self-similarity speedup.</p>
<hr>

LanguageLearner.java
<pre>
package ai;

import ai.model.EntropyGraph;
import ai.model.WordGraph;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.LineNumberReader;
import java.io.PrintStream;
import java.util.HashSet;
import java.util.Set;

class LanguageLearner {

    final EntropyGraph mAssocGraph;
    final String mDelim;

    LanguageLearner(EntropyGraph assocGraph, String delim) {
        mAssocGraph = assocGraph;
        mDelim = delim;
    }

    void listen(LineNumberReader lnr) throws IOException {
        String line;
        while ((line = lnr.readLine()) != null) {
            final String [] words = line.split(mDelim);
            for (int i = 0; i < words.length; i++) {
                final String word = words[i];
                if (word.length() == 0)
                    continue;
                if (i == 0)
                    mAssocGraph.linkDirectional(word, word);
                else if (i == words.length - 1)
                    mAssocGraph.linkDirectional(word, word);
                else {
                    mAssocGraph.linkDirectional(word, words[i - 1]);
                    mAssocGraph.linkDirectional(word, words[i + 1]);
                }
            }
        }
    }

    /**
     * Prints the maximum entropy path through the graph.
     */
    void speak(PrintStream ps) {
        speak(mAssocGraph.maxEntropyNode(), ps, new HashSet());
    }

    /**
     * Recursively prints the maximum entropy path from the given
     * node, without looping through nodes in the visted set.  When
     * this method recurses, the given word has been added to the
     * visted set.
     */
    void speak(String word, PrintStream ps, Set visited) {
        if (word == null || visited.contains(word))
            return;
        ps.print(word + " ");
        visited.add(word);
        speak(mAssocGraph.highestEntropyNeighbor(word), ps, visited);
    }

    public static void main(String [] args) throws IOException {
        final WordGraph wordNet = new WordGraph(); // hahah.
        final LanguageLearner l = new LanguageLearner(wordNet, "\\s+");
        l.listen(new LineNumberReader(new InputStreamReader(System.in)));
        l.speak(System.out);
    }
}
</pre>
