<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
  <head>
    <meta name="Author" lang="en" content="Pablo Mayrgundter">
    <meta name="description" content="The culture we take to the stars.">
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <meta name="keywords" content="a few of my favorite things, free, freedom, public, civic, music, independent, indie, food, technology, culture, the cuture we take to the stars, rant, wish, linux, java, jane's addiction, empire, government, gang, philosophy, religion">
    <title>Pablo's Code</title>
    <link rel="stylesheet" href="http://www.freality.com/style.css" type="text/css" media="screen">
    <link rel="stylesheet" href="../personal.css" type="text/css" media="screen">
    <link rel="shortcut icon" href="/freality.ico" type="image/x-icon">
  </head>
  <body>
    <h2>Machine Learning</h2>
    <p>In the beginning, there was an immediate love affair between
    computer scientists and the idea of creating a thinking machine.
    Alan Turing, one of the first computer scientists, speculated in a
    concrete way about how and when computers would achieve the
    capability to think like humans (he figured the test of
    intelligence that he proposed would be achieved by a computer by
    the year 2000).  Since then, the race has been on to figure out
    just how we think or what thought is in general, and then how to
    emulate that kind of thought in a computer.

    <p>In the 50s and 60s some AI researchers were closing in on the
    development of computer programs that could automatically solve
    algebraic expressions that were very difficult to solve by hand.
    The ability to "understand" math in this way was considered by
    these researchers to be the very epitome of what it is to think.
    In another area of study, new understandings of how the human
    brain works led to the development of the idea of a "Perceptron",
    a simple mathematical model analogous to how the brain's neurons
    might work.  As these ideas matured, claims began to be made that
    human-like intelligence would be a trivial next-step.  And
    everyone held their breath, and waited, and waited.. and
    waited.</p>

    <p>The problem has proven to be deceptively hard.  The term
    "Artificial Intelligence" itself became too loaded to bet grant
    proposals on, and the entire field of study took a collective step
    back.  Such research is now less directed at "intelligence" per
    se, and more at what is simply called "learning".  "Machine
    Learning": a kinder, gentler AI research.</p>
    
    <p>Instead of tackling Intelligence as a whole, including
    perception, knowledge, creativity, emotion and other facets of
    thought that are part of our nature, ML is more concerned with
    knowledge representation in a mathematical/logical/statistical
    sense.  The <em>spirit</em> of Intelligence is no longer an object
    of study; we are content for the moment in simply emulatating its
    basest <em>capabilities</em>.</p>

    <p>Once the paradigm shifted, life in Computer Science academia
    got back to normal (there were also other things, like GUIs, PCs
    and the Net to work on instead) and things continued this way up
    until now.  Or at least that's how I heard it.</p>

    <p>But who can really turn away from such a seductive siren?  If
    you're studying Machine Learning by day, odds are you're
    pondering AI by night.</p>

    <p>After all, the general rhetorical failure of early AI research
    wasn't truly for naught.  Although seemingly fruitless in its
    stated goals, five decades of AI research has exposed a vast field
    of study, and has revealed some promising clues in pursuit of a
    better understanding of intelligence.</p>

    <p>One such clue is the delicate balance between the
    <em>general</em> and the <em>specific</em> aspects of knowledge
    that is needed for effective learning.  To learn from experience,
    it does not suffice to simply remember all the details of an event
    and then try to apply those exact details to new events.  Nor is
    this solved by generalizing the learning process to arbitrarily
    abstract categories and models.  The trick is to come to a
    balance, a hybrid, of salient details on the one hand, and general
    structure on the other.  Ideally, new events will fit a known
    general structure and be contextualized by salient details.  With
    such a learning ability, it has been possible to emulate some of
    the characteristic capabilities of intelligence, such as pattern
    recognition and event prediciton.</p>

    <p>I have begun to implement some of the key ML algorithms here as
    a study (and to help me ponder AI by night :).  Feel free to take or join
    in.</p>

  </body>
</html>

